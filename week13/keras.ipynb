{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.6"},"colab":{"name":"keras.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UWlH5cFCFbjO","executionInfo":{"status":"ok","timestamp":1608013722051,"user_tz":-480,"elapsed":23868,"user":{"displayName":"N26091194鄧立昌","photoUrl":"","userId":"09046790946486703509"}},"outputId":"ea0a57e0-b92d-41e0-ce5c-46ba8a80cc8e"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8hnXtwdv0pHc"},"source":["# Keras\n","\n","[Keras](https://keras.io/) 是一種較 Tensorflow 更高階的深度學習框架，可以使用更少量的程式碼來建立深度學習模型，可以先熟悉 [Tensorflow 單元](/notebooks/unit/tensorflow/tenforflow.ipynb)後再來閱讀本單元。Keras 使用更低階的深度學習框架作為後端引擎，目前支援如 CNTK、Tensorflow、Theano 等知名框架。本單元將介紹 Keras 中 Model 與 Layer 的用法，並實作一個圖片分類器。\n","\n","## 1. Model & Layer\n","\n","在 Keras，可以宣告一個 [Model](https://keras.io/models/about-keras-models/) 物件，並透過加入一層一層的 [Layer](https://keras.io/layers/about-keras-layers/) 來建構一個神經網路，神經網路的運算(例如訓練)都可以透過該 Model 物件來操作。下方程式區段使用了 Keras 中常見的 [Sequential Model](https://keras.io/models/sequential/)，並加入了四種 Layer：\n","\n","1. [Convolutional Layer](https://keras.io/layers/convolutional/)：卷積層在影像、圖片應用上，表現比全連結層(Keras 的 Dense Layer 更為優異)，參考[卷積神經網絡介紹](https://medium.com/@yehjames/4f8249d65d4f)。\n","2. [Pooling Layer](https://keras.io/layers/pooling/#maxpooling2d)：池化層的工作是降採樣(down sampling)，以下方程式區段使用的 MaxPooling 為例，將每個 2x2 降採樣為該區域的最大值。\n","3. [Flatten Layer](https://keras.io/layers/core/#flatten)：將原本多維度的資料拉平成一維，目的是讓前一層的輸出可以接到下一層(通常是全連接層)的輸入。\n","4. [Dense Layer](https://keras.io/layers/core/#dense)：全連結層。  "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z4JvVifv0pHc","executionInfo":{"status":"ok","timestamp":1607407631588,"user_tz":-480,"elapsed":6463,"user":{"displayName":"N26091194鄧立昌","photoUrl":"","userId":"09046790946486703509"}},"outputId":"36dd66b0-91bc-40fc-a8ef-83e7f30aaac2"},"source":["from keras.models import Sequential\n","from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\n","\n","model = Sequential() # Declare a sequential model\n","\n","# Add a 2D convolutional layer with 64 nodes, a 3x3 filter and relu as avtivation function\n","# After this layer, `model.output_shape` is (None, 62, 62, 64)\n","model.add(Conv2D(64, (3, 3), activation='relu', input_shape=(64, 64, 3)))\n","\n","# Add a 2D max pooling layer that pools the maximun value every 2x2 area\n","# After this layer, `model.output_shape` is (None, 31, 31, 64)\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","# Add a flatten layer\n","# After this layer, `model.output_shape` is (None, 61504)\n","model.add(Flatten())\n","\n","# Add a dense layer with 32 nodes and sigmoid as activation function\n","# After this layer, `model.output_shape` is (None, 32)\n","model.add(Dense(32, activation='sigmoid'))\n","\n","# See `model`\n","model.summary() "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d (Conv2D)              (None, 62, 62, 64)        1792      \n","_________________________________________________________________\n","max_pooling2d (MaxPooling2D) (None, 31, 31, 64)        0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 61504)             0         \n","_________________________________________________________________\n","dense (Dense)                (None, 32)                1968160   \n","=================================================================\n","Total params: 1,969,952\n","Trainable params: 1,969,952\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3AXCz1rZ0pHd"},"source":["## 2. CIFAR-10\n","\n","CIFAR 的全名為 Canadian Institute for Advanced Research，是由加拿大政府出資並由多位科學家、工程師收集而成的圖片資料庫。[CIFAR-10](http://www.cs.toronto.edu/~kriz/cifar.html) 包含 60000 張 32x32x3 的 RGB 彩色圖片，其中 50000 張為訓練資料，10000 張為測試資料。CIFAR-10 有 10 種類別，0~9 分別對應為：\n","\n","```\n","airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck\n","```\n","\n","![CIFAR-10](./cifar_10.png)\n","\n","Keras 提供[整理好的 CIFAR-10 資料](https://keras.io/datasets/#cifar10-small-image-classification)，只要透過 `import` 就可以拿到對應的訓練與測試資料。用法如下："]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":339},"id":"q73XrnAF0pHd","executionInfo":{"status":"ok","timestamp":1607419256335,"user_tz":-480,"elapsed":1569,"user":{"displayName":"N26091194鄧立昌","photoUrl":"","userId":"09046790946486703509"}},"outputId":"688ef0ee-5a57-434f-f2d3-c60636975c3a"},"source":["%matplotlib inline\n","\n","import matplotlib.pyplot as plt\n","import keras\n","from keras.datasets import cifar10\n","\n","(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","\n","# see the data shape\n","print(x_train.shape, y_train.shape)\n","print(x_test.shape, y_test.shape)\n","print()\n","\n","# show the i-th sample of the cifar-10 training set, try a different `i`\n","i = 0\n","plt.imshow(x_train[i])\n","print('The label of training sample %d is %s.' % (i, y_train[i]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(50000, 32, 32, 3) (50000, 1)\n","(10000, 32, 32, 3) (10000, 1)\n","\n","The label of training sample 0 is [6].\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"bNPwu0Er0pHd"},"source":["## 3. 範例模型\n","\n","以下使用 Keras 實作兩種深度學習模型來進行 CIFAR-10 圖片分類。其中 DNN 只使用全連接層，而 CNN 多使用了卷積層。相較於 [Tensorflow 單元](/notebooks/unit/tensorflow/tenforflow.ipynb)的 MNIST 資料，CIFAR-10 的圖片比較複雜且為彩色，更能發揮卷積層的效果。也因此相較於 DNN，CNN 應該更容易得到好的結果。注意比較以下兩個程式區段，使用 CNN 時不需要將圖片 reshape 為一維向量。當然，CNN 也能處理 reshape 過的一維向量。CNN 的初學者可以參考以下連結：\n","\n","* [A Beginner's Guide To Understanding Convolutional Neural Networks](https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/)\n","* [深度學習(2)--使用Tensorflow實作卷積神經網路(Convolutional neural network，CNN)](http://arbu00.blogspot.tw/2017/03/2-tensorflowconvolutional-neural.html)\n","\n","![Convolutional Neural Network](https://adeshpande3.github.io/assets/Cover.png)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dWcQF5kY0pHd","executionInfo":{"status":"ok","timestamp":1607405473617,"user_tz":-480,"elapsed":19183,"user":{"displayName":"N26091194鄧立昌","photoUrl":"","userId":"09046790946486703509"}},"outputId":"2856822b-7659-407c-a763-386a302a1de0"},"source":["# DNN\n","\n","from keras.utils import to_categorical\n","from keras.models import Sequential\n","from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\n","\n","x_tr_dnn = x_train[:10000].astype('float32')\n","x_te_dnn = x_test.astype('float32')\n","\n","# note that the CNN version does not need to reshape the input\n","x_tr_dnn = x_tr_dnn.reshape(-1, 3072)\n","x_te_dnn = x_te_dnn.reshape(-1, 3072)\n","\n","# normalize\n","x_tr_dnn /= 255\n","x_te_dnn /= 255\n","\n","# one-hot encoding\n","y_tr_dnn = to_categorical(y_train[:10000], num_classes=10)\n","y_te_dnn = to_categorical(y_test, num_classes=10)\n","\n","# options\n","epochs = 20\n","batch_size = 128 \n","learning_rate = 0.001\n","\n","# model\n","model = Sequential()\n","model.add(Dense(100, activation='relu', input_shape=(3072,)))\n","model.add(Dense(10, activation='softmax'))\n","model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n","\n","# train\n","model.fit(x_tr_dnn, y_tr_dnn, batch_size=batch_size, epochs=epochs, shuffle=True, validation_data=(x_te_dnn, y_te_dnn))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/20\n","79/79 [==============================] - 1s 14ms/step - loss: 2.1756 - accuracy: 0.2278 - val_loss: 1.9899 - val_accuracy: 0.2806\n","Epoch 2/20\n","79/79 [==============================] - 1s 10ms/step - loss: 1.9212 - accuracy: 0.3169 - val_loss: 1.8848 - val_accuracy: 0.3312\n","Epoch 3/20\n","79/79 [==============================] - 1s 11ms/step - loss: 1.8558 - accuracy: 0.3402 - val_loss: 1.9150 - val_accuracy: 0.3259\n","Epoch 4/20\n","79/79 [==============================] - 1s 11ms/step - loss: 1.8353 - accuracy: 0.3504 - val_loss: 1.8331 - val_accuracy: 0.3533\n","Epoch 5/20\n","79/79 [==============================] - 1s 11ms/step - loss: 1.7961 - accuracy: 0.3670 - val_loss: 1.8621 - val_accuracy: 0.3363\n","Epoch 6/20\n","79/79 [==============================] - 1s 11ms/step - loss: 1.7791 - accuracy: 0.3703 - val_loss: 1.8002 - val_accuracy: 0.3614\n","Epoch 7/20\n","79/79 [==============================] - 1s 10ms/step - loss: 1.7605 - accuracy: 0.3753 - val_loss: 1.7924 - val_accuracy: 0.3665\n","Epoch 8/20\n","79/79 [==============================] - 1s 11ms/step - loss: 1.7320 - accuracy: 0.3931 - val_loss: 1.7719 - val_accuracy: 0.3686\n","Epoch 9/20\n","79/79 [==============================] - 1s 11ms/step - loss: 1.7251 - accuracy: 0.3920 - val_loss: 1.8678 - val_accuracy: 0.3432\n","Epoch 10/20\n","79/79 [==============================] - 1s 11ms/step - loss: 1.7112 - accuracy: 0.3962 - val_loss: 1.7983 - val_accuracy: 0.3698\n","Epoch 11/20\n","79/79 [==============================] - 1s 11ms/step - loss: 1.6844 - accuracy: 0.4033 - val_loss: 1.7689 - val_accuracy: 0.3680\n","Epoch 12/20\n","79/79 [==============================] - 1s 11ms/step - loss: 1.6777 - accuracy: 0.4113 - val_loss: 1.7745 - val_accuracy: 0.3749\n","Epoch 13/20\n","79/79 [==============================] - 1s 11ms/step - loss: 1.6633 - accuracy: 0.4148 - val_loss: 1.7401 - val_accuracy: 0.3827\n","Epoch 14/20\n","79/79 [==============================] - 1s 11ms/step - loss: 1.6574 - accuracy: 0.4156 - val_loss: 1.7532 - val_accuracy: 0.3762\n","Epoch 15/20\n","79/79 [==============================] - 1s 11ms/step - loss: 1.6459 - accuracy: 0.4204 - val_loss: 1.7583 - val_accuracy: 0.3829\n","Epoch 16/20\n","79/79 [==============================] - 1s 11ms/step - loss: 1.6326 - accuracy: 0.4276 - val_loss: 1.7504 - val_accuracy: 0.3846\n","Epoch 17/20\n","79/79 [==============================] - 1s 11ms/step - loss: 1.6409 - accuracy: 0.4213 - val_loss: 1.7359 - val_accuracy: 0.3903\n","Epoch 18/20\n","79/79 [==============================] - 1s 10ms/step - loss: 1.6343 - accuracy: 0.4221 - val_loss: 1.7862 - val_accuracy: 0.3643\n","Epoch 19/20\n","79/79 [==============================] - 1s 10ms/step - loss: 1.6285 - accuracy: 0.4207 - val_loss: 1.7302 - val_accuracy: 0.3829\n","Epoch 20/20\n","79/79 [==============================] - 1s 11ms/step - loss: 1.5955 - accuracy: 0.4357 - val_loss: 1.8513 - val_accuracy: 0.3467\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f4fdeecf6d8>"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5c9QjOpi0pHd","executionInfo":{"status":"ok","timestamp":1607407776454,"user_tz":-480,"elapsed":10273,"user":{"displayName":"N26091194鄧立昌","photoUrl":"","userId":"09046790946486703509"}},"outputId":"fdf09a27-b4fd-4458-81b3-dee9ef2ffad9"},"source":["# CNN\n","\n","from keras.utils import to_categorical\n","from keras.models import Sequential\n","from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\n","\n","x_tr_cnn = x_train[:10000].astype('float32')\n","x_te_cnn = x_test.astype('float32')\n","\n","# normalize\n","x_tr_cnn /= 255\n","x_te_cnn /= 255\n","\n","# one-hot encoding\n","y_tr_cnn = to_categorical(y_train[:10000], num_classes=10)\n","y_te_cnn = to_categorical(y_test, num_classes=10)\n","\n","# options\n","epochs = 20\n","batch_size = 128 \n","learning_rate = 0.001\n","\n","# model\n","model = Sequential()\n","\n","# the input shape for cifar-10 is (32, 32, 3)\n","# use `Conv2D(#neurons, (filter_size))` to add convolutionary layers\n","model.add(Conv2D(32, (3, 3), input_shape=(32, 32, 3), activation='relu'))\n","\n","# use `MaxPooling2D()` to add pooling layers\n","# model.add(MaxPooling2D((2, 2)))\n","\n","# TODO: add more convolutionary and/or pooling layers here\n","\n","# in practice, fully-connected layers are added after convolutionary and pooling ones \n","model.add(Flatten())\n","model.add(Dense(10, activation='softmax'))\n","model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n","\n","# train\n","model.fit(x_tr_cnn, y_tr_cnn, batch_size=batch_size, epochs=epochs, shuffle=True, validation_data=(x_te_cnn, y_te_cnn))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/20\n","79/79 [==============================] - 1s 8ms/step - loss: 1.8555 - accuracy: 0.3478 - val_loss: 1.6765 - val_accuracy: 0.4141\n","Epoch 2/20\n","79/79 [==============================] - 0s 6ms/step - loss: 1.5077 - accuracy: 0.4776 - val_loss: 1.5525 - val_accuracy: 0.4459\n","Epoch 3/20\n","79/79 [==============================] - 0s 6ms/step - loss: 1.3285 - accuracy: 0.5464 - val_loss: 1.4425 - val_accuracy: 0.4930\n","Epoch 4/20\n","79/79 [==============================] - 0s 5ms/step - loss: 1.1887 - accuracy: 0.5928 - val_loss: 1.4183 - val_accuracy: 0.4913\n","Epoch 5/20\n","79/79 [==============================] - 0s 6ms/step - loss: 1.0977 - accuracy: 0.6271 - val_loss: 1.3732 - val_accuracy: 0.5104\n","Epoch 6/20\n","79/79 [==============================] - 0s 6ms/step - loss: 0.9959 - accuracy: 0.6678 - val_loss: 1.4084 - val_accuracy: 0.5024\n","Epoch 7/20\n","79/79 [==============================] - 0s 6ms/step - loss: 0.9332 - accuracy: 0.6888 - val_loss: 1.3980 - val_accuracy: 0.5125\n","Epoch 8/20\n","79/79 [==============================] - 0s 6ms/step - loss: 0.8616 - accuracy: 0.7162 - val_loss: 1.3890 - val_accuracy: 0.5188\n","Epoch 9/20\n","79/79 [==============================] - 0s 6ms/step - loss: 0.8153 - accuracy: 0.7319 - val_loss: 1.4008 - val_accuracy: 0.5231\n","Epoch 10/20\n","79/79 [==============================] - 0s 6ms/step - loss: 0.7581 - accuracy: 0.7564 - val_loss: 1.4036 - val_accuracy: 0.5244\n","Epoch 11/20\n","79/79 [==============================] - 0s 6ms/step - loss: 0.7136 - accuracy: 0.7739 - val_loss: 1.4150 - val_accuracy: 0.5199\n","Epoch 12/20\n","79/79 [==============================] - 0s 6ms/step - loss: 0.6617 - accuracy: 0.7946 - val_loss: 1.4337 - val_accuracy: 0.5177\n","Epoch 13/20\n","79/79 [==============================] - 0s 5ms/step - loss: 0.6222 - accuracy: 0.8107 - val_loss: 1.4768 - val_accuracy: 0.5178\n","Epoch 14/20\n","79/79 [==============================] - 0s 6ms/step - loss: 0.5907 - accuracy: 0.8226 - val_loss: 1.4823 - val_accuracy: 0.5101\n","Epoch 15/20\n","79/79 [==============================] - 0s 6ms/step - loss: 0.5566 - accuracy: 0.8346 - val_loss: 1.5275 - val_accuracy: 0.5090\n","Epoch 16/20\n","79/79 [==============================] - 0s 6ms/step - loss: 0.5149 - accuracy: 0.8511 - val_loss: 1.5385 - val_accuracy: 0.5125\n","Epoch 17/20\n","79/79 [==============================] - 0s 6ms/step - loss: 0.4894 - accuracy: 0.8620 - val_loss: 1.5753 - val_accuracy: 0.5112\n","Epoch 18/20\n","79/79 [==============================] - 0s 6ms/step - loss: 0.4517 - accuracy: 0.8763 - val_loss: 1.5785 - val_accuracy: 0.5162\n","Epoch 19/20\n","79/79 [==============================] - 0s 6ms/step - loss: 0.4188 - accuracy: 0.8918 - val_loss: 1.6186 - val_accuracy: 0.5100\n","Epoch 20/20\n","79/79 [==============================] - 0s 6ms/step - loss: 0.3997 - accuracy: 0.8971 - val_loss: 1.6199 - val_accuracy: 0.5110\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f8c940f7240>"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"utM1faQ90pHd"},"source":["## 4. 作業\n","若依照以上範例的設定，經過 20 輪訓練後，DNN 可以到達約 40% 的正確率，而 CNN 可以達到約 50% 的正確率。請更改 DNN 或是 CNN 的架構來改善模型。validation 的準確度即為本次作業分數\n","\n","提示：\n","1. 調整訓練輪數(`epochs`)\n","2. 調整批次大小(`batch_size`)\n","3. 調整學習速率(`learning_rate`)\n","4. 增加層數\n","5. 調整每層的神經元數量\n","6. 參考經典網路架構"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s6BMpecJTCDm","executionInfo":{"status":"ok","timestamp":1607419946952,"user_tz":-480,"elapsed":4074,"user":{"displayName":"N26091194鄧立昌","photoUrl":"","userId":"09046790946486703509"}},"outputId":"51aa9a93-02d0-46de-d8ab-db5a170c7af8"},"source":["# CNN\n","import keras\n","import matplotlib.pyplot as plt\n","from keras.utils import to_categorical\n","from keras.models import Sequential\n","from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten, BatchNormalization, Activation\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras import regularizers\n","from keras.callbacks import LearningRateScheduler\n","from keras.datasets import cifar10\n","\n","def lr_schedule(epoch):\n","    lrate = 0.001\n","    if epoch > 75:\n","        lrate = 0.0005\n","    if epoch > 100:\n","        lrate = 0.0003\n","    return lrate\n","\n","(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","x_train = x_train.astype('float32')\n","x_test = x_test.astype('float32')\n","\n","# normalize\n","x_train /= 255\n","x_test /= 255\n","\n","# one-hot encoding\n","num_classes = 10\n","y_train = to_categorical(y_train, num_classes)\n","y_test = to_categorical(y_test, num_classes)\n","\n","\n","# model\n","keras.backend.clear_session()\n","weight_decay = 1e-4\n","\n","#============================================================================================\n","model = Sequential()\n","model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=x_train.shape[1:]))\n","model.add(Activation('elu'))\n","model.add(BatchNormalization())\n","model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n","model.add(Activation('elu'))\n","model.add(BatchNormalization())\n","model.add(MaxPooling2D(pool_size=(2,2)))\n","model.add(Dropout(0.2))\n"," \n","model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n","model.add(Activation('elu'))\n","model.add(BatchNormalization())\n","model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n","model.add(Activation('elu'))\n","model.add(BatchNormalization())\n","model.add(MaxPooling2D(pool_size=(2,2)))\n","model.add(Dropout(0.3))\n"," \n","model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n","model.add(Activation('elu'))\n","model.add(BatchNormalization())\n","model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n","model.add(Activation('elu'))\n","model.add(BatchNormalization())\n","model.add(MaxPooling2D(pool_size=(2,2)))\n","model.add(Dropout(0.4))\n"," \n","model.add(Flatten())\n","model.add(Dense(num_classes, activation='softmax'))\n","#============================================================================================\n","\n","model.summary()\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d (Conv2D)              (None, 32, 32, 32)        896       \n","_________________________________________________________________\n","activation (Activation)      (None, 32, 32, 32)        0         \n","_________________________________________________________________\n","batch_normalization (BatchNo (None, 32, 32, 32)        128       \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            (None, 32, 32, 32)        9248      \n","_________________________________________________________________\n","activation_1 (Activation)    (None, 32, 32, 32)        0         \n","_________________________________________________________________\n","batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n","_________________________________________________________________\n","max_pooling2d (MaxPooling2D) (None, 16, 16, 32)        0         \n","_________________________________________________________________\n","dropout (Dropout)            (None, 16, 16, 32)        0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 16, 16, 64)        18496     \n","_________________________________________________________________\n","activation_2 (Activation)    (None, 16, 16, 64)        0         \n","_________________________________________________________________\n","batch_normalization_2 (Batch (None, 16, 16, 64)        256       \n","_________________________________________________________________\n","conv2d_3 (Conv2D)            (None, 16, 16, 64)        36928     \n","_________________________________________________________________\n","activation_3 (Activation)    (None, 16, 16, 64)        0         \n","_________________________________________________________________\n","batch_normalization_3 (Batch (None, 16, 16, 64)        256       \n","_________________________________________________________________\n","max_pooling2d_1 (MaxPooling2 (None, 8, 8, 64)          0         \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 8, 8, 64)          0         \n","_________________________________________________________________\n","conv2d_4 (Conv2D)            (None, 8, 8, 128)         73856     \n","_________________________________________________________________\n","activation_4 (Activation)    (None, 8, 8, 128)         0         \n","_________________________________________________________________\n","batch_normalization_4 (Batch (None, 8, 8, 128)         512       \n","_________________________________________________________________\n","conv2d_5 (Conv2D)            (None, 8, 8, 128)         147584    \n","_________________________________________________________________\n","activation_5 (Activation)    (None, 8, 8, 128)         0         \n","_________________________________________________________________\n","batch_normalization_5 (Batch (None, 8, 8, 128)         512       \n","_________________________________________________________________\n","max_pooling2d_2 (MaxPooling2 (None, 4, 4, 128)         0         \n","_________________________________________________________________\n","dropout_2 (Dropout)          (None, 4, 4, 128)         0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 2048)              0         \n","_________________________________________________________________\n","dense (Dense)                (None, 10)                20490     \n","=================================================================\n","Total params: 309,290\n","Trainable params: 308,394\n","Non-trainable params: 896\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LWA9CEQAW2Cc","executionInfo":{"status":"ok","timestamp":1607422818465,"user_tz":-480,"elapsed":2869105,"user":{"displayName":"N26091194鄧立昌","photoUrl":"","userId":"09046790946486703509"}},"outputId":"88569e87-58df-4052-b9a3-7bddc7f35ded"},"source":["#data augmentation\n","datagen = ImageDataGenerator(\n","    rotation_range=15,\n","    width_shift_range=0.1,\n","    height_shift_range=0.1,\n","    horizontal_flip=True,\n","    )\n","datagen.fit(x_train)\n","\n","#training\n","batch_size = 128\n","\n","opt_rms = keras.optimizers.RMSprop(lr=0.001,decay=1e-6)\n","model.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])\n","history=model.fit(datagen.flow(x_train, y_train, batch_size=batch_size),\\\n","                    steps_per_epoch=x_train.shape[0] // batch_size,epochs=125,\\\n","                    verbose=1,validation_data=(x_test,y_test),callbacks=[LearningRateScheduler(lr_schedule)])\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/125\n","390/390 [==============================] - 23s 60ms/step - loss: 2.0124 - accuracy: 0.3955 - val_loss: 1.5330 - val_accuracy: 0.4695\n","Epoch 2/125\n","390/390 [==============================] - 23s 58ms/step - loss: 1.3782 - accuracy: 0.5512 - val_loss: 1.6652 - val_accuracy: 0.5285\n","Epoch 3/125\n","390/390 [==============================] - 23s 59ms/step - loss: 1.1498 - accuracy: 0.6246 - val_loss: 1.0166 - val_accuracy: 0.6776\n","Epoch 4/125\n","390/390 [==============================] - 23s 58ms/step - loss: 1.0149 - accuracy: 0.6706 - val_loss: 1.3695 - val_accuracy: 0.6120\n","Epoch 5/125\n","390/390 [==============================] - 23s 58ms/step - loss: 0.9384 - accuracy: 0.6981 - val_loss: 0.9603 - val_accuracy: 0.6972\n","Epoch 6/125\n","390/390 [==============================] - 23s 58ms/step - loss: 0.8812 - accuracy: 0.7178 - val_loss: 0.8723 - val_accuracy: 0.7298\n","Epoch 7/125\n","390/390 [==============================] - 23s 58ms/step - loss: 0.8391 - accuracy: 0.7354 - val_loss: 0.9041 - val_accuracy: 0.7366\n","Epoch 8/125\n","390/390 [==============================] - 23s 58ms/step - loss: 0.8104 - accuracy: 0.7476 - val_loss: 0.7828 - val_accuracy: 0.7759\n","Epoch 9/125\n","390/390 [==============================] - 23s 59ms/step - loss: 0.7806 - accuracy: 0.7591 - val_loss: 0.7432 - val_accuracy: 0.7865\n","Epoch 10/125\n","390/390 [==============================] - 22s 57ms/step - loss: 0.7542 - accuracy: 0.7706 - val_loss: 0.7732 - val_accuracy: 0.7769\n","Epoch 11/125\n","390/390 [==============================] - 22s 57ms/step - loss: 0.7393 - accuracy: 0.7752 - val_loss: 0.8205 - val_accuracy: 0.7593\n","Epoch 12/125\n","390/390 [==============================] - 23s 58ms/step - loss: 0.7246 - accuracy: 0.7813 - val_loss: 0.7082 - val_accuracy: 0.7972\n","Epoch 13/125\n","390/390 [==============================] - 22s 57ms/step - loss: 0.7132 - accuracy: 0.7865 - val_loss: 0.7037 - val_accuracy: 0.8003\n","Epoch 14/125\n","390/390 [==============================] - 23s 58ms/step - loss: 0.6893 - accuracy: 0.7941 - val_loss: 0.6997 - val_accuracy: 0.8025\n","Epoch 15/125\n","390/390 [==============================] - 23s 58ms/step - loss: 0.6796 - accuracy: 0.8001 - val_loss: 0.7014 - val_accuracy: 0.8028\n","Epoch 16/125\n","390/390 [==============================] - 23s 58ms/step - loss: 0.6748 - accuracy: 0.8033 - val_loss: 0.8020 - val_accuracy: 0.7737\n","Epoch 17/125\n","390/390 [==============================] - 22s 58ms/step - loss: 0.6596 - accuracy: 0.8070 - val_loss: 0.6937 - val_accuracy: 0.8057\n","Epoch 18/125\n","390/390 [==============================] - 22s 58ms/step - loss: 0.6560 - accuracy: 0.8096 - val_loss: 0.7012 - val_accuracy: 0.8041\n","Epoch 19/125\n","390/390 [==============================] - 23s 58ms/step - loss: 0.6473 - accuracy: 0.8126 - val_loss: 0.6432 - val_accuracy: 0.8246\n","Epoch 20/125\n","390/390 [==============================] - 23s 58ms/step - loss: 0.6345 - accuracy: 0.8198 - val_loss: 0.6190 - val_accuracy: 0.8297\n","Epoch 21/125\n","390/390 [==============================] - 22s 57ms/step - loss: 0.6301 - accuracy: 0.8216 - val_loss: 0.7041 - val_accuracy: 0.8080\n","Epoch 22/125\n","390/390 [==============================] - 23s 58ms/step - loss: 0.6245 - accuracy: 0.8235 - val_loss: 0.6532 - val_accuracy: 0.8181\n","Epoch 23/125\n","390/390 [==============================] - 24s 61ms/step - loss: 0.6229 - accuracy: 0.8232 - val_loss: 0.6843 - val_accuracy: 0.8142\n","Epoch 24/125\n","390/390 [==============================] - 23s 59ms/step - loss: 0.6142 - accuracy: 0.8276 - val_loss: 0.6054 - val_accuracy: 0.8374\n","Epoch 25/125\n","390/390 [==============================] - 23s 59ms/step - loss: 0.6119 - accuracy: 0.8307 - val_loss: 0.7273 - val_accuracy: 0.8024\n","Epoch 26/125\n","390/390 [==============================] - 23s 60ms/step - loss: 0.6075 - accuracy: 0.8280 - val_loss: 0.7082 - val_accuracy: 0.8094\n","Epoch 27/125\n","390/390 [==============================] - 23s 59ms/step - loss: 0.6033 - accuracy: 0.8312 - val_loss: 0.6073 - val_accuracy: 0.8400\n","Epoch 28/125\n","390/390 [==============================] - 23s 58ms/step - loss: 0.5993 - accuracy: 0.8336 - val_loss: 0.5920 - val_accuracy: 0.8430\n","Epoch 29/125\n","390/390 [==============================] - 23s 59ms/step - loss: 0.5950 - accuracy: 0.8372 - val_loss: 0.5706 - val_accuracy: 0.8514\n","Epoch 30/125\n","390/390 [==============================] - 23s 58ms/step - loss: 0.5909 - accuracy: 0.8374 - val_loss: 0.6236 - val_accuracy: 0.8306\n","Epoch 31/125\n","390/390 [==============================] - 23s 58ms/step - loss: 0.5843 - accuracy: 0.8390 - val_loss: 0.6527 - val_accuracy: 0.8268\n","Epoch 32/125\n","390/390 [==============================] - 23s 59ms/step - loss: 0.5760 - accuracy: 0.8420 - val_loss: 0.6002 - val_accuracy: 0.8423\n","Epoch 33/125\n","390/390 [==============================] - 24s 60ms/step - loss: 0.5775 - accuracy: 0.8427 - val_loss: 0.5443 - val_accuracy: 0.8559\n","Epoch 34/125\n","390/390 [==============================] - 23s 59ms/step - loss: 0.5768 - accuracy: 0.8428 - val_loss: 0.6592 - val_accuracy: 0.8351\n","Epoch 35/125\n","390/390 [==============================] - 23s 60ms/step - loss: 0.5759 - accuracy: 0.8440 - val_loss: 0.5752 - val_accuracy: 0.8494\n","Epoch 36/125\n","390/390 [==============================] - 24s 61ms/step - loss: 0.5771 - accuracy: 0.8440 - val_loss: 0.6654 - val_accuracy: 0.8256\n","Epoch 37/125\n","390/390 [==============================] - 23s 58ms/step - loss: 0.5715 - accuracy: 0.8456 - val_loss: 0.6530 - val_accuracy: 0.8274\n","Epoch 38/125\n","390/390 [==============================] - 23s 58ms/step - loss: 0.5683 - accuracy: 0.8445 - val_loss: 0.6504 - val_accuracy: 0.8290\n","Epoch 39/125\n","390/390 [==============================] - 23s 58ms/step - loss: 0.5652 - accuracy: 0.8483 - val_loss: 0.6026 - val_accuracy: 0.8432\n","Epoch 40/125\n","390/390 [==============================] - 23s 58ms/step - loss: 0.5592 - accuracy: 0.8498 - val_loss: 0.7253 - val_accuracy: 0.8186\n","Epoch 41/125\n","390/390 [==============================] - 23s 59ms/step - loss: 0.5627 - accuracy: 0.8490 - val_loss: 0.5655 - val_accuracy: 0.8517\n","Epoch 42/125\n","390/390 [==============================] - 23s 59ms/step - loss: 0.5613 - accuracy: 0.8495 - val_loss: 0.6146 - val_accuracy: 0.8390\n","Epoch 43/125\n","390/390 [==============================] - 23s 60ms/step - loss: 0.5611 - accuracy: 0.8478 - val_loss: 0.6116 - val_accuracy: 0.8405\n","Epoch 44/125\n","390/390 [==============================] - 23s 58ms/step - loss: 0.5558 - accuracy: 0.8520 - val_loss: 0.5670 - val_accuracy: 0.8565\n","Epoch 45/125\n","390/390 [==============================] - 23s 59ms/step - loss: 0.5596 - accuracy: 0.8509 - val_loss: 0.5700 - val_accuracy: 0.8540\n","Epoch 46/125\n","390/390 [==============================] - 23s 59ms/step - loss: 0.5526 - accuracy: 0.8525 - val_loss: 0.5499 - val_accuracy: 0.8535\n","Epoch 47/125\n","390/390 [==============================] - 23s 60ms/step - loss: 0.5514 - accuracy: 0.8547 - val_loss: 0.8226 - val_accuracy: 0.7914\n","Epoch 48/125\n","390/390 [==============================] - 23s 60ms/step - loss: 0.5499 - accuracy: 0.8535 - val_loss: 0.5701 - val_accuracy: 0.8538\n","Epoch 49/125\n","390/390 [==============================] - 24s 61ms/step - loss: 0.5503 - accuracy: 0.8537 - val_loss: 0.6458 - val_accuracy: 0.8323\n","Epoch 50/125\n","390/390 [==============================] - 24s 63ms/step - loss: 0.5445 - accuracy: 0.8565 - val_loss: 0.5615 - val_accuracy: 0.8547\n","Epoch 51/125\n","390/390 [==============================] - 24s 61ms/step - loss: 0.5473 - accuracy: 0.8543 - val_loss: 0.5863 - val_accuracy: 0.8478\n","Epoch 52/125\n","390/390 [==============================] - 23s 60ms/step - loss: 0.5503 - accuracy: 0.8558 - val_loss: 0.5400 - val_accuracy: 0.8649\n","Epoch 53/125\n","390/390 [==============================] - 24s 60ms/step - loss: 0.5448 - accuracy: 0.8567 - val_loss: 0.5609 - val_accuracy: 0.8565\n","Epoch 54/125\n","390/390 [==============================] - 23s 60ms/step - loss: 0.5440 - accuracy: 0.8559 - val_loss: 0.5692 - val_accuracy: 0.8553\n","Epoch 55/125\n","390/390 [==============================] - 23s 59ms/step - loss: 0.5422 - accuracy: 0.8585 - val_loss: 0.6155 - val_accuracy: 0.8463\n","Epoch 56/125\n","390/390 [==============================] - 23s 59ms/step - loss: 0.5386 - accuracy: 0.8572 - val_loss: 0.5732 - val_accuracy: 0.8530\n","Epoch 57/125\n","390/390 [==============================] - 23s 60ms/step - loss: 0.5357 - accuracy: 0.8603 - val_loss: 0.5533 - val_accuracy: 0.8600\n","Epoch 58/125\n","390/390 [==============================] - 23s 60ms/step - loss: 0.5410 - accuracy: 0.8560 - val_loss: 0.5457 - val_accuracy: 0.8648\n","Epoch 59/125\n","390/390 [==============================] - 23s 59ms/step - loss: 0.5364 - accuracy: 0.8601 - val_loss: 0.6517 - val_accuracy: 0.8280\n","Epoch 60/125\n","390/390 [==============================] - 23s 59ms/step - loss: 0.5359 - accuracy: 0.8584 - val_loss: 0.5848 - val_accuracy: 0.8483\n","Epoch 61/125\n","390/390 [==============================] - 23s 59ms/step - loss: 0.5342 - accuracy: 0.8590 - val_loss: 0.5863 - val_accuracy: 0.8500\n","Epoch 62/125\n","390/390 [==============================] - 23s 59ms/step - loss: 0.5353 - accuracy: 0.8602 - val_loss: 0.5664 - val_accuracy: 0.8539\n","Epoch 63/125\n","390/390 [==============================] - 24s 60ms/step - loss: 0.5360 - accuracy: 0.8595 - val_loss: 0.5681 - val_accuracy: 0.8581\n","Epoch 64/125\n","390/390 [==============================] - 23s 60ms/step - loss: 0.5314 - accuracy: 0.8627 - val_loss: 0.5466 - val_accuracy: 0.8604\n","Epoch 65/125\n","390/390 [==============================] - 23s 60ms/step - loss: 0.5273 - accuracy: 0.8634 - val_loss: 0.6143 - val_accuracy: 0.8433\n","Epoch 66/125\n","390/390 [==============================] - 23s 59ms/step - loss: 0.5290 - accuracy: 0.8620 - val_loss: 0.5660 - val_accuracy: 0.8563\n","Epoch 67/125\n","390/390 [==============================] - 23s 60ms/step - loss: 0.5286 - accuracy: 0.8627 - val_loss: 0.5563 - val_accuracy: 0.8571\n","Epoch 68/125\n","390/390 [==============================] - 23s 60ms/step - loss: 0.5247 - accuracy: 0.8618 - val_loss: 0.6229 - val_accuracy: 0.8434\n","Epoch 69/125\n","390/390 [==============================] - 23s 60ms/step - loss: 0.5240 - accuracy: 0.8635 - val_loss: 0.5609 - val_accuracy: 0.8559\n","Epoch 70/125\n","390/390 [==============================] - 23s 60ms/step - loss: 0.5266 - accuracy: 0.8642 - val_loss: 0.6311 - val_accuracy: 0.8383\n","Epoch 71/125\n","390/390 [==============================] - 23s 60ms/step - loss: 0.5261 - accuracy: 0.8648 - val_loss: 0.5982 - val_accuracy: 0.8566\n","Epoch 72/125\n","390/390 [==============================] - 23s 59ms/step - loss: 0.5252 - accuracy: 0.8631 - val_loss: 0.5475 - val_accuracy: 0.8629\n","Epoch 73/125\n","390/390 [==============================] - 22s 57ms/step - loss: 0.5245 - accuracy: 0.8645 - val_loss: 0.5885 - val_accuracy: 0.8519\n","Epoch 74/125\n","390/390 [==============================] - 22s 57ms/step - loss: 0.5243 - accuracy: 0.8639 - val_loss: 0.6344 - val_accuracy: 0.8353\n","Epoch 75/125\n","390/390 [==============================] - 22s 57ms/step - loss: 0.5248 - accuracy: 0.8637 - val_loss: 0.5825 - val_accuracy: 0.8587\n","Epoch 76/125\n","390/390 [==============================] - 23s 58ms/step - loss: 0.5243 - accuracy: 0.8650 - val_loss: 0.5567 - val_accuracy: 0.8645\n","Epoch 77/125\n","390/390 [==============================] - 22s 58ms/step - loss: 0.4852 - accuracy: 0.8764 - val_loss: 0.4953 - val_accuracy: 0.8811\n","Epoch 78/125\n","390/390 [==============================] - 22s 58ms/step - loss: 0.4728 - accuracy: 0.8803 - val_loss: 0.5060 - val_accuracy: 0.8781\n","Epoch 79/125\n","390/390 [==============================] - 22s 57ms/step - loss: 0.4649 - accuracy: 0.8838 - val_loss: 0.4991 - val_accuracy: 0.8781\n","Epoch 80/125\n","390/390 [==============================] - 23s 58ms/step - loss: 0.4611 - accuracy: 0.8840 - val_loss: 0.5412 - val_accuracy: 0.8657\n","Epoch 81/125\n","390/390 [==============================] - 23s 58ms/step - loss: 0.4529 - accuracy: 0.8864 - val_loss: 0.4756 - val_accuracy: 0.8852\n","Epoch 82/125\n","390/390 [==============================] - 23s 58ms/step - loss: 0.4537 - accuracy: 0.8864 - val_loss: 0.5320 - val_accuracy: 0.8653\n","Epoch 83/125\n","390/390 [==============================] - 23s 58ms/step - loss: 0.4482 - accuracy: 0.8871 - val_loss: 0.5300 - val_accuracy: 0.8697\n","Epoch 84/125\n","390/390 [==============================] - 23s 58ms/step - loss: 0.4486 - accuracy: 0.8861 - val_loss: 0.5421 - val_accuracy: 0.8597\n","Epoch 85/125\n","390/390 [==============================] - 23s 59ms/step - loss: 0.4401 - accuracy: 0.8878 - val_loss: 0.4889 - val_accuracy: 0.8799\n","Epoch 86/125\n","390/390 [==============================] - 23s 59ms/step - loss: 0.4444 - accuracy: 0.8860 - val_loss: 0.5487 - val_accuracy: 0.8636\n","Epoch 87/125\n","390/390 [==============================] - 23s 59ms/step - loss: 0.4377 - accuracy: 0.8876 - val_loss: 0.4663 - val_accuracy: 0.8851\n","Epoch 88/125\n","390/390 [==============================] - 23s 60ms/step - loss: 0.4460 - accuracy: 0.8844 - val_loss: 0.5065 - val_accuracy: 0.8725\n","Epoch 89/125\n","390/390 [==============================] - 23s 59ms/step - loss: 0.4365 - accuracy: 0.8869 - val_loss: 0.5396 - val_accuracy: 0.8647\n","Epoch 90/125\n","390/390 [==============================] - 23s 59ms/step - loss: 0.4332 - accuracy: 0.8893 - val_loss: 0.4914 - val_accuracy: 0.8790\n","Epoch 91/125\n","390/390 [==============================] - 22s 58ms/step - loss: 0.4354 - accuracy: 0.8868 - val_loss: 0.4879 - val_accuracy: 0.8745\n","Epoch 92/125\n","390/390 [==============================] - 23s 58ms/step - loss: 0.4335 - accuracy: 0.8879 - val_loss: 0.4819 - val_accuracy: 0.8778\n","Epoch 93/125\n","390/390 [==============================] - 23s 58ms/step - loss: 0.4326 - accuracy: 0.8885 - val_loss: 0.4437 - val_accuracy: 0.8883\n","Epoch 94/125\n","390/390 [==============================] - 23s 58ms/step - loss: 0.4289 - accuracy: 0.8895 - val_loss: 0.4794 - val_accuracy: 0.8813\n","Epoch 95/125\n","390/390 [==============================] - 23s 58ms/step - loss: 0.4265 - accuracy: 0.8893 - val_loss: 0.4806 - val_accuracy: 0.8800\n","Epoch 96/125\n","390/390 [==============================] - 23s 59ms/step - loss: 0.4240 - accuracy: 0.8886 - val_loss: 0.5247 - val_accuracy: 0.8683\n","Epoch 97/125\n","390/390 [==============================] - 23s 58ms/step - loss: 0.4282 - accuracy: 0.8888 - val_loss: 0.4901 - val_accuracy: 0.8804\n","Epoch 98/125\n","390/390 [==============================] - 23s 58ms/step - loss: 0.4240 - accuracy: 0.8896 - val_loss: 0.4811 - val_accuracy: 0.8776\n","Epoch 99/125\n","390/390 [==============================] - 23s 59ms/step - loss: 0.4218 - accuracy: 0.8905 - val_loss: 0.4975 - val_accuracy: 0.8737\n","Epoch 100/125\n","390/390 [==============================] - 23s 59ms/step - loss: 0.4205 - accuracy: 0.8909 - val_loss: 0.4844 - val_accuracy: 0.8814\n","Epoch 101/125\n","390/390 [==============================] - 23s 59ms/step - loss: 0.4180 - accuracy: 0.8911 - val_loss: 0.5334 - val_accuracy: 0.8692\n","Epoch 102/125\n","390/390 [==============================] - 23s 60ms/step - loss: 0.4026 - accuracy: 0.8954 - val_loss: 0.4844 - val_accuracy: 0.8779\n","Epoch 103/125\n","390/390 [==============================] - 23s 59ms/step - loss: 0.3971 - accuracy: 0.8989 - val_loss: 0.4778 - val_accuracy: 0.8800\n","Epoch 104/125\n","390/390 [==============================] - 23s 59ms/step - loss: 0.3930 - accuracy: 0.8983 - val_loss: 0.4528 - val_accuracy: 0.8857\n","Epoch 105/125\n","390/390 [==============================] - 22s 57ms/step - loss: 0.3911 - accuracy: 0.8991 - val_loss: 0.4760 - val_accuracy: 0.8770\n","Epoch 106/125\n","390/390 [==============================] - 22s 57ms/step - loss: 0.3861 - accuracy: 0.9010 - val_loss: 0.5174 - val_accuracy: 0.8713\n","Epoch 107/125\n","390/390 [==============================] - 23s 58ms/step - loss: 0.3864 - accuracy: 0.9003 - val_loss: 0.4605 - val_accuracy: 0.8863\n","Epoch 108/125\n","390/390 [==============================] - 22s 57ms/step - loss: 0.3853 - accuracy: 0.9009 - val_loss: 0.4594 - val_accuracy: 0.8853\n","Epoch 109/125\n","390/390 [==============================] - 23s 58ms/step - loss: 0.3821 - accuracy: 0.9003 - val_loss: 0.4648 - val_accuracy: 0.8831\n","Epoch 110/125\n","390/390 [==============================] - 23s 58ms/step - loss: 0.3765 - accuracy: 0.9035 - val_loss: 0.4444 - val_accuracy: 0.8877\n","Epoch 111/125\n","390/390 [==============================] - 22s 57ms/step - loss: 0.3809 - accuracy: 0.9007 - val_loss: 0.4866 - val_accuracy: 0.8780\n","Epoch 112/125\n","390/390 [==============================] - 22s 57ms/step - loss: 0.3793 - accuracy: 0.8999 - val_loss: 0.4288 - val_accuracy: 0.8906\n","Epoch 113/125\n","390/390 [==============================] - 22s 57ms/step - loss: 0.3750 - accuracy: 0.9031 - val_loss: 0.4769 - val_accuracy: 0.8802\n","Epoch 114/125\n","390/390 [==============================] - 22s 57ms/step - loss: 0.3740 - accuracy: 0.9036 - val_loss: 0.4403 - val_accuracy: 0.8894\n","Epoch 115/125\n","390/390 [==============================] - 22s 57ms/step - loss: 0.3752 - accuracy: 0.9032 - val_loss: 0.4550 - val_accuracy: 0.8812\n","Epoch 116/125\n","390/390 [==============================] - 22s 57ms/step - loss: 0.3760 - accuracy: 0.9023 - val_loss: 0.4342 - val_accuracy: 0.8906\n","Epoch 117/125\n","390/390 [==============================] - 23s 58ms/step - loss: 0.3747 - accuracy: 0.9012 - val_loss: 0.4397 - val_accuracy: 0.8882\n","Epoch 118/125\n","390/390 [==============================] - 22s 57ms/step - loss: 0.3747 - accuracy: 0.9033 - val_loss: 0.4610 - val_accuracy: 0.8873\n","Epoch 119/125\n","390/390 [==============================] - 22s 57ms/step - loss: 0.3718 - accuracy: 0.9032 - val_loss: 0.4613 - val_accuracy: 0.8827\n","Epoch 120/125\n","390/390 [==============================] - 22s 58ms/step - loss: 0.3679 - accuracy: 0.9046 - val_loss: 0.4460 - val_accuracy: 0.8878\n","Epoch 121/125\n","390/390 [==============================] - 22s 57ms/step - loss: 0.3708 - accuracy: 0.9024 - val_loss: 0.4243 - val_accuracy: 0.8941\n","Epoch 122/125\n","390/390 [==============================] - 22s 57ms/step - loss: 0.3673 - accuracy: 0.9047 - val_loss: 0.5044 - val_accuracy: 0.8734\n","Epoch 123/125\n","390/390 [==============================] - 22s 58ms/step - loss: 0.3692 - accuracy: 0.9044 - val_loss: 0.4927 - val_accuracy: 0.8753\n","Epoch 124/125\n","390/390 [==============================] - 23s 58ms/step - loss: 0.3705 - accuracy: 0.9037 - val_loss: 0.4742 - val_accuracy: 0.8794\n","Epoch 125/125\n","390/390 [==============================] - 22s 58ms/step - loss: 0.3646 - accuracy: 0.9036 - val_loss: 0.4660 - val_accuracy: 0.8814\n"],"name":"stdout"}]}]}