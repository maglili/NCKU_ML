{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection & Extraction\n",
    "\n",
    "How much information you provide for model really matters. In Machine Learning, we often have to deal with the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality) issue. For example, the following image shows a practical model which delivers worse performance as the dimension increases. Feature selection and extraction are two common methods to solve this issue.\n",
    "\n",
    "![optimal dimension](http://scikit-learn.org/stable/_images/sphx_glr_plot_rfe_with_cross_validation_001.png)\n",
    "\n",
    "## Sequential Feature Selection\n",
    "\n",
    "According to the [Wikipedia](https://en.wikipedia.org/wiki/Feature_selection):\n",
    "\n",
    "> In machine learning and statistics, feature selection, also known as variable selection, attribute selection or variable subset selection, is the process of selecting a subset of relevant features (variables, predictors) for use in model construction. Feature selection techniques are used for four reasons:\n",
    "\n",
    "> * simplification of models to make them easier to interpret by researchers/users,\n",
    "> * shorter training times,\n",
    "> * to avoid the curse of dimensionality,\n",
    "> * enhanced generalization by reducing overfitting (formally, reduction of variance)\n",
    "\n",
    "For example, if you have a 20D dataset and you want to visualize it in a 2D plot. You can use feature selection to pick two features with the highest importance. Another practical benefit is that, with less features, you can train your model in less time. Following are two common methods for feature selection: **backward selection** and **forward selection**. You can check [other methods available on sklearn](http://scikit-learn.org/stable/modules/feature_selection.html).\n",
    "\n",
    "### Backward selection\n",
    "\n",
    "In backward selection, we starts from all features and remove them one-by-one. Following is a backwrod selection example on a 20D dataset:\n",
    "    \n",
    "1. We create 20 19D-datasets by removing each feature indiviudially and use them to generate 20 models.\n",
    " \n",
    "```\n",
    "( fea_02, fea_03, fea_04, fea_05, .... fea_20 ) -> acc_01\n",
    "( fea_01, fea_03, fea_04, fea_05, .... fea_20 ) -> acc_02\n",
    "  ....\n",
    "( fea_01, fea_02, fea_03, fea_04, .... fea_19 ) -> acc_20\n",
    "```\n",
    "       \n",
    "2. Eliminate the feature corresponding to the model with the highest performance, since the deletion of it is \"least\" important.\n",
    "    \n",
    "```\n",
    "feature_index_we_dont_want = argmax( [acc_01, acc_02, ..., acc_20] )\n",
    "del features[ feature_index_we_dont_want ]\n",
    "```\n",
    "        \n",
    "3. Repeat above steps.\n",
    "\n",
    "\n",
    "### Forward selection\n",
    "\n",
    "In comparison with backward selection, forward selection starts from an empty list and add features one-by-one. Each time we add a feature which contributes to performance most.\n",
    "    \n",
    "1. We create 20 1D-datasets by adding each feature indiviudially and use them to generate 20 models.\n",
    "\n",
    "```\n",
    "( fea_01 ) -> acc_01\n",
    "( fea_02 ) -> acc_02\n",
    "  ....\n",
    "( fea_20 ) -> acc_20\n",
    "```\n",
    "\n",
    "2. Add the feature corresponding to the model with the highest performance, since the addition of it is \"most\" important.\n",
    "\n",
    "```\n",
    "feature_index_we_want = argmmax( [acc_01, acc_02, ..., acc_20] )\n",
    "features_we_want.append( feature_index_we_want )\n",
    "```\n",
    "\n",
    "3. Repeat above steps.\n",
    "    \n",
    "## Feature selection using SelectFromModel¶\n",
    "> The features are considered unimportant and removed, if the corresponding **coef_** or **feature_importances_** values are below the provided threshold parameter.\n",
    "\n",
    "> Note: Apart from specifying the threshold numerically, there are built-in heuristics for finding a threshold using a string argument. Available heuristics are “mean”, “median” and float multiples of these like “0.1*mean”.\n",
    "\n",
    "### L1-based feature selection\n",
    "> Linear models penalized with the L1 norm have sparse solutions: many of their estimated coefficients are zero. When the goal is to reduce the dimensionality of the data to use with another classifier, they can be used along with [feature_selection.SelectFromModel](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel) to select the non-zero coefficients. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "print(X.shape)\n",
    "lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(X, y)\n",
    "model = SelectFromModel(lsvc, prefit=True)\n",
    "X_new = model.transform(X)\n",
    "print(X_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree-based feature selection\n",
    "> Tree-based estimators (see the [sklearn.tree](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.tree) module and forest of trees in the [sklearn.ensemble](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble) module) can be used to compute feature importances, which in turn can be used to discard irrelevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "print(X.shape)\n",
    "\n",
    "clf = ExtraTreesClassifier()\n",
    "clf = clf.fit(X, y)\n",
    "clf.feature_importances_  \n",
    "\n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "X_new = model.transform(X)\n",
    "print(X_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "According to the [Wikipedia](https://en.wikipedia.org/wiki/Feature_extraction):\n",
    "\n",
    "> In machine learning, pattern recognition and in image processing, feature extraction starts from an initial set of measured data and builds derived values (features) intended to be informative and non-redundant, facilitating the subsequent learning and generalization steps, and in some cases leading to better human interpretations. Feature extraction is a dimensionality reduction process, where an initial set of raw variables is reduced to more manageable groups (features) for processing, while still accurately and completely describing the original data set.\n",
    "\n",
    "In comparison with feature selection, feature extraction may create new features of which a new feature could be a linear combination of several existing ones. The most famous feature extraction algorithm is [Principal Component Analysis](https://en.wikipedia.org/wiki/Principal_component_analysis). You may see [other feature extraction algorithms](https://www.sciencedirect.com/science/article/pii/0031320371900033). The code below shows a PCA example with sklearn: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original shape is  (500, 20) , the shape after extraction is  (500, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "import numpy as np\n",
    "\n",
    "original_x, y = make_blobs(n_samples=500, centers=3, n_features=20)\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(original_x)\n",
    "reduced_x = pca.transform(original_x)\n",
    "print(\"original shape is \", original_x.shape, \", the shape after extraction is \", reduced_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using k-fold cross validation to assess model performance\n",
    "\n",
    "### The holdout method\n",
    "![image.png](./holdout.png)\n",
    "### K-fold cross-validation\n",
    "![image.png](./img.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('.prepared')\n",
    "import demo as prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Fill in the TODO segements in the following code cell, which uses backward selection to filter out 18 features and leave 2 features.  You can use accuracy of either 10-fold corss-validation or train-test to select features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select by accuracy of cross validation (10 fold)\n",
      "Remain features: [10 16]\n",
      "\n",
      "Select by accuracy on testing data\n",
      "\n",
      "Remain features: [13 16]\n",
      "[epoch 0]\n",
      "delete feature: 0\n",
      "remaining feature index: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "--------------------\n",
      "[epoch 1]\n",
      "delete feature: 0\n",
      "remaining feature index: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "--------------------\n",
      "[epoch 2]\n",
      "delete feature: 0\n",
      "remaining feature index: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "--------------------\n",
      "[epoch 3]\n",
      "delete feature: 1\n",
      "remaining feature index: [0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "--------------------\n",
      "[epoch 4]\n",
      "delete feature: 1\n",
      "remaining feature index: [0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "--------------------\n",
      "[epoch 5]\n",
      "delete feature: 1\n",
      "remaining feature index: [0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "--------------------\n",
      "[epoch 6]\n",
      "delete feature: 1\n",
      "remaining feature index: [0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "--------------------\n",
      "[epoch 7]\n",
      "delete feature: 3\n",
      "remaining feature index: [0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "--------------------\n",
      "[epoch 8]\n",
      "delete feature: 0\n",
      "remaining feature index: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "--------------------\n",
      "[epoch 9]\n",
      "delete feature: 1\n",
      "remaining feature index: [0, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "--------------------\n",
      "[epoch 10]\n",
      "delete feature: 7\n",
      "remaining feature index: [0, 1, 2, 3, 4, 5, 6, 8, 9]\n",
      "--------------------\n",
      "[epoch 11]\n",
      "delete feature: 2\n",
      "remaining feature index: [0, 1, 3, 4, 5, 6, 7, 8]\n",
      "--------------------\n",
      "[epoch 12]\n",
      "delete feature: 0\n",
      "remaining feature index: [1, 2, 3, 4, 5, 6, 7]\n",
      "--------------------\n",
      "[epoch 13]\n",
      "delete feature: 0\n",
      "remaining feature index: [1, 2, 3, 4, 5, 6]\n",
      "--------------------\n",
      "[epoch 14]\n",
      "delete feature: 1\n",
      "remaining feature index: [0, 2, 3, 4, 5]\n",
      "--------------------\n",
      "[epoch 15]\n",
      "delete feature: 1\n",
      "remaining feature index: [0, 2, 3, 4]\n",
      "--------------------\n",
      "[epoch 16]\n",
      "delete feature: 3\n",
      "remaining feature index: [0, 1, 2]\n",
      "--------------------\n",
      "[epoch 17]\n",
      "delete feature: 2\n",
      "remaining feature index: [0, 1]\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "n_features = 20\n",
    "x, y = make_blobs(n_samples=500, centers=3, n_features=n_features, random_state=3, cluster_std=5)\n",
    "classifier = GaussianNB() # in order to save time and computation power, please fix the model to GNB\n",
    "\n",
    "print('Select by accuracy of cross validation (10 fold)')\n",
    "prepared.select_by_cv(classifier, x, y, 2)\n",
    "\n",
    "print()\n",
    "print('Select by accuracy on testing data')\n",
    "print()\n",
    "train_x, train_y, test_x, test_y = x[:400], y[:400], x[400:], y[400:]\n",
    "prepared.select_by_test_accuracy(classifier, train_x, train_y, test_x, test_y, 2)\n",
    "\n",
    "# you should implement the function like Recursive Feature Elimination in sklearn\n",
    "# http://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_with_cross_validation.html\n",
    " \n",
    "# external loop to remove feature once at a time\n",
    "for n_remove_fea in range(0, n_features - 2):\n",
    "    score_set=[]\n",
    "    print('[epoch '+str(n_remove_fea)+']')\n",
    "    # internal loop to compare the performance of each feature\n",
    "    for i in range(0, n_features - n_remove_fea):\n",
    "        extract_train_x = np.delete(train_x, i, axis=1)\n",
    "        classifier.fit(extract_train_x, train_y)\n",
    "        extract_test_x = np.delete(test_x, i, axis=1)\n",
    "        score = classifier.score(X=extract_test_x, y=test_y)\n",
    "        #print('feature {:<2} score: {:>2}'.format(i, score))\n",
    "        score_set.append(score)\n",
    "        #pass # comment this line after filling in the TODO segments\n",
    "\n",
    "    # TODO: use np.argmax to get the least important feature and delete it and print the remaining feature index\n",
    "    det_fea_ind = np.argmax(score_set)\n",
    "    print('delete feature:',det_fea_ind)\n",
    "    print('remaining feature index:',[ i for i in range(train_x.shape[1]) if i != det_fea_ind])\n",
    "    print('-'*20)\n",
    "    train_x =np.delete(train_x, det_fea_ind, axis = 1)\n",
    "    test_x = np.delete(test_x, det_fea_ind, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-15.85285511,   8.41813481])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select by accuracy of cross validation (10 fold)\n",
      "Remain features: [10 16]\n",
      "\n",
      "Select by accuracy on testing data\n",
      "\n",
      "Remain features: [13 16]\n",
      "[epoch 0]\n",
      "[1 4 6 2 1 1 1 1 1 1 1 1 3 1 1 1 1 5 1 1]\n",
      "[1 4 6 2 1 1 1 1 1 1 1 1 3 1 1 1 1 5 1 1]\n",
      "[1 4 6 2 1 1 1 1 1 1 1 1 3 1 1 1 1 5 1 1]\n",
      "[1 4 6 2 1 1 1 1 1 1 1 1 3 1 1 1 1 5 1 1]\n",
      "[1 4 6 2 1 1 1 1 1 1 1 1 3 1 1 1 1 5 1 1]\n",
      "[1 4 6 2 1 1 1 1 1 1 1 1 3 1 1 1 1 5 1 1]\n",
      "[1 4 6 2 1 1 1 1 1 1 1 1 3 1 1 1 1 5 1 1]\n",
      "[1 4 6 2 1 1 1 1 1 1 1 1 3 1 1 1 1 5 1 1]\n",
      "[1 4 6 2 1 1 1 1 1 1 1 1 3 1 1 1 1 5 1 1]\n",
      "[1 4 6 2 1 1 1 1 1 1 1 1 3 1 1 1 1 5 1 1]\n",
      "[1 4 6 2 1 1 1 1 1 1 1 1 3 1 1 1 1 5 1 1]\n",
      "[1 4 6 2 1 1 1 1 1 1 1 1 3 1 1 1 1 5 1 1]\n",
      "[1 4 6 2 1 1 1 1 1 1 1 1 3 1 1 1 1 5 1 1]\n",
      "[1 4 6 2 1 1 1 1 1 1 1 1 3 1 1 1 1 5 1 1]\n",
      "[1 4 6 2 1 1 1 1 1 1 1 1 3 1 1 1 1 5 1 1]\n",
      "[1 4 6 2 1 1 1 1 1 1 1 1 3 1 1 1 1 5 1 1]\n",
      "[1 4 6 2 1 1 1 1 1 1 1 1 3 1 1 1 1 5 1 1]\n",
      "[1 4 6 2 1 1 1 1 1 1 1 1 3 1 1 1 1 5 1 1]\n",
      "[1 4 6 2 1 1 1 1 1 1 1 1 3 1 1 1 1 5 1 1]\n",
      "[1 4 6 2 1 1 1 1 1 1 1 1 3 1 1 1 1 5 1 1]\n",
      "[epoch 1]\n",
      "[1 4 6 2 1 1 1 1 1 1 1 1 3 1 1 1 1 5 1 1]\n",
      "[1 4 6 2 1 1 1 1 1 1 1 1 3 1 1 1 1 5 1 1]\n",
      "[1 4 6 2 1 1 1 1 1 1 1 1 3 1 1 1 1 5 1 1]\n",
      "[1 4 6 2 1 1 1 1 1 1 1 1 3 1 1 1 1 5 1 1]\n",
      "[1 4 6 2 1 1 1 1 1 1 1 1 3 1 1 1 1 5 1 1]\n",
      "[1 4 6 2 1 1 1 1 1 1 1 1 3 1 1 1 1 5 1 1]\n",
      "[1 4 6 2 1 1 1 1 1 1 1 1 3 1 1 1 1 5 1 1]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-1edc451a902c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m#pass # comment this line after filling in the TODO segments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mrfecv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRFECV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msvc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mrfecv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrfecv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mranking_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\feature_selection\\_rfe.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdelayed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_rfe_single_fit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m         scores = parallel(\n\u001b[0m\u001b[0;32m    551\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrfe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             for train, test in cv.split(X, y, groups))\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\feature_selection\\_rfe.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m         scores = parallel(\n\u001b[1;32m--> 551\u001b[1;33m             \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrfe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    552\u001b[0m             for train, test in cv.split(X, y, groups))\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\feature_selection\\_rfe.py\u001b[0m in \u001b[0;36m_rfe_single_fit\u001b[1;34m(rfe, estimator, X, y, train, test, scorer)\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_safe_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_safe_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     return rfe._fit(\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         _score(estimator, X_test[:, features], y_test, scorer)).scores_\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\feature_selection\\_rfe.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, step_score)\u001b[0m\n\u001b[0;32m    194\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Fitting estimator with %d features.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msupport_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m             \u001b[1;31m# Get coefs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m         \u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m         \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m         \u001b[1;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msupport_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msupport_vectors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_support\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdual_coef_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintercept_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_probA\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m             self._probB, self.fit_status_ = libsvm.fit(\n\u001b[0m\u001b[0;32m    269\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m                 \u001b[0msvm_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "n_features = 20\n",
    "x, y = make_blobs(n_samples=500, centers=3, n_features=n_features, random_state=3, cluster_std=5)\n",
    "classifier = GaussianNB() # in order to save time and computation power, please fix the model to GNB\n",
    "\n",
    "print('Select by accuracy of cross validation (10 fold)')\n",
    "prepared.select_by_cv(classifier, x, y, 2)\n",
    "\n",
    "print()\n",
    "print('Select by accuracy on testing data')\n",
    "print()\n",
    "train_x, train_y, test_x, test_y = x[:400], y[:400], x[400:], y[400:]\n",
    "prepared.select_by_test_accuracy(classifier, train_x, train_y, test_x, test_y, 2)\n",
    "\n",
    "# you should implement the function like Recursive Feature Elimination in sklearn\n",
    "# http://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_with_cross_validation.html\n",
    " \n",
    "svc = SVC(kernel=\"linear\")    \n",
    "# external loop to remove feature once at a time\n",
    "for n_remove_fea in range(0, n_features - 2):\n",
    "    score_set=[]\n",
    "    print('[epoch '+str(n_remove_fea)+']')\n",
    "    # internal loop to compare the performance of each feature\n",
    "    for i in range(0, n_features - n_remove_fea):\n",
    "        #pass # comment this line after filling in the TODO segments\n",
    "        rfecv = RFECV(estimator=svc, step=1, cv=10,scoring='accuracy')\n",
    "        rfecv.fit(train_x, train_y)\n",
    "        score = rfecv.ranking_\n",
    "        print(score)\n",
    "\n",
    "    # TODO: use np.argmax to get the least important feature and delete it and print the remaining feature index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
